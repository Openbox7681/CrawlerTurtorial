{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named mysql_schema",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8c69a1e66cc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmysql_schema\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmysql_conn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfake_useragent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUserAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named mysql_schema"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# import HTMLParser\n",
    "# import logging\n",
    "# import os\n",
    "# import mysql_schema as mysql_conn\n",
    "# import ConfigParser\n",
    "# from fake_useragent import UserAgent\n",
    "\n",
    "# es = Elasticsearch(\"211.23.163.51:9200\", timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class chinatimes():\n",
    "    \n",
    "    # 解析\n",
    "    def parse(self):\n",
    "        URL_main = \"https://www.bnext.com.tw/\"\n",
    "        response = requests.get(URL_main)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soup_body=soup.find(\"div\", {\"class\":\"menu_box row\"}).find_all(\"div\", {\"class\":\"title_sty01\"})\n",
    "        #取得分類\n",
    "        class_lists=[]\n",
    "        a=0\n",
    "        milli_sec = int(round(time.time() * 1000))\n",
    "        print(milli_sec)\n",
    "        for body in soup_body: #將網頁分類用回圈找出全部的分類網址\n",
    "            class_lists.append(URL_main + body.find(\"a\").get(\"href\"))\n",
    "            a=a+1\n",
    "            if a==10:#分類到第十一筆是空的所以在地時比跳出回圈\n",
    "                break\n",
    "#         self.parse_2(class_lists)#將class_lists陳列資料都到parse_2方式中\n",
    "        \n",
    "        \n",
    "    def parse_2(self , class_lists):\n",
    "    \n",
    "        for s in class_lists:\n",
    "            response = requests.get(s)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            soup_body=soup.find(\"div\", {\"class\":\"left\"}).find_all(\"div\",{\"class\":\"item_inner\"})\n",
    "            driver= webdriver.Chrome()#模擬網頁\n",
    "            driver.get(s)\n",
    "            time.sleep(2)#等待二秒\n",
    "            try :\n",
    "                noad=driver.find_element_by_class_name('bx_icon')#關閉廣告\n",
    "                noad.click()#關閉廣告\n",
    "            except:\n",
    "                time.sleep(0.1)\n",
    "            a=0\n",
    "            lateTime=[]\n",
    "            for body in soup_body:\n",
    "                lateTime.append(body.find(\"div\",{\"class\":\"div_td td1\"}).text)\n",
    "#             lowtime = datetime.datetime.strptime(lateTime[-1], \"%Y-%m-%d\")\n",
    "#             print lowtime \n",
    "\n",
    "            while a<1:\n",
    "                try :\n",
    "                    lowtime = datetime.datetime.strptime(lateTime[-1], \"%Y-%m-%d\")\n",
    "                    print lowtime\n",
    "                    if time_start<=lowtime:\n",
    "                        button = driver.find_element_by_class_name('more_btn')#更多內容\n",
    "                        button.click()#更多內容\n",
    "                        time.sleep(3)#等待3秒\n",
    "                        html_source = driver.page_source#重新刷新HTML內容\n",
    "                        time.sleep(3)#等待3秒\n",
    "                        soup = BeautifulSoup(html_source, 'lxml')#重新抓取HTML內容\n",
    "                        soup_body=soup.find(\"div\", {\"class\":\"left\"}).find_all(\"div\",{\"class\":\"item_inner\"})\n",
    "                        for body in soup_body:\n",
    "                            lateTime.append(body.find(\"div\",{\"class\":\"div_td td1\"}).text)\n",
    "                    else:\n",
    "                        a=a+2\n",
    "                except:\n",
    "                    button = driver.find_element_by_class_name('more_btn')#更多內容\n",
    "                    button.click()#更多內容\n",
    "                    time.sleep(3)#等待3秒\n",
    "                    html_source = driver.page_source#重新刷新HTML內容\n",
    "                    time.sleep(3)#等待3秒\n",
    "                    soup = BeautifulSoup(html_source, 'lxml')#重新抓取HTML內容\n",
    "                    soup_body=soup.find(\"div\", {\"class\":\"left\"}).find_all(\"div\",{\"class\":\"item_inner\"})\n",
    "            \n",
    "            class_lists_paper=[]\n",
    "            for body in soup_body:\n",
    "                class_lists_paper.append(body.find(\"a\",{\"class\":\"item_img bg_img_sty01\"}).get(\"href\"))\n",
    "                lateTime.append(body.find(\"div\",{\"class\":\"div_td td1\"}).text)\n",
    "            \n",
    "            for aa in class_lists_paper:\n",
    "                print aa\n",
    "#             print lateTime[-1] \n",
    "            class_name=soup.find(\"div\",{\"class\":\"title\"}).text\n",
    "            self.parse_3(class_lists_paper ,class_name ,time_start ,time_end)\n",
    "           \n",
    "    def parse_3(self , class_lists_paper ,class_name , time_start ,time_end):\n",
    "        result_json = dict()\n",
    "        for d in class_lists_paper:\n",
    "            print\"++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "            response = requests.get(d)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            Article_title=soup.find(\"title\")\n",
    "            Article_time=soup.find(\"span\",{\"class\":\"item\"}).text\n",
    "            Article_timenew = datetime.datetime.strptime(Article_time, \"%Y.%m.%d\")\n",
    "            Article_acontent=soup.find(\"article\",{\"class\":\"main_content\"}).find_all(\"p\")\n",
    "            try :\n",
    "                Article_tag=soup.find(\"div\",{\"class\":\"article_tags\"}).find_all(\"a\")\n",
    "            except:\n",
    "                Article_tag=\"\"\n",
    "            index=\"info_chinatimes_\"\n",
    "            types=\"chinatimes\"\n",
    "            tags=\"\"\n",
    "            acontentall=\"\"\n",
    "            url=d.split(\"/\", 5)\n",
    "            \n",
    "            for content in Article_acontent:\n",
    "                acontentall=acontentall+\"\\n\"+content.text\n",
    "            for tag in Article_tag:\n",
    "                tags=tags+tag.text\n",
    "            if time_end>=Article_timenew:\n",
    "                if time_start<= Article_timenew:\n",
    "                    result_json[\"Title\"] = Article_title.text\n",
    "                    result_json[\"Class\"] = class_name\n",
    "                    result_json[\"URL\"] = d\n",
    "                    result_json[\"Publish_Time\"] = Article_time\n",
    "                    result_json[\"Description\"] = acontentall\n",
    "\n",
    "\n",
    "        #             print result_json[\"Context\"]\n",
    "                    # 傳入資料庫\n",
    "                    try:\n",
    "                        index = index+Article_time.text[:7]\n",
    "                        id=Article_time.text+\"_\"+url[4]\n",
    "                        res = es.index(index=index, doc_type=\"chinatimes\", body=json.dumps(result_json, encoding=\"UTF-8\", ensure_ascii=False), id=id)\n",
    "                        print res[\"result\"]\n",
    "                    except:\n",
    "                        print 'save fail'\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=chinatimes()\n",
    "aaa.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
